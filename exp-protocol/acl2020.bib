@misc{cer2018universal,
    title={Universal Sentence Encoder},
    author={Daniel Cer and Yinfei Yang and Sheng-yi Kong and Nan Hua and Nicole Limtiaco and Rhomni St. John and Noah Constant and Mario Guajardo-Cespedes and Steve Yuan and Chris Tar and Yun-Hsuan Sung and Brian Strope and Ray Kurzweil},
    year={2018},
    eprint={1803.11175},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{clement2019use,
    title={On the Use of ArXiv as a Dataset},
    author={Colin B. Clement and Matthew Bierbaum and Kevin P. O'Keeffe and Alexander A. Alemi},
    year={2019},
    eprint={1905.00075},
    archivePrefix={arXiv},
    primaryClass={cs.IR}
}

@misc{devlin2018bert,
    title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year={2018},
    eprint={1810.04805},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{gong2019document,
    title={Document Similarity for Texts of Varying Lengths via Hidden Topics},
    author={Hongyu Gong and Tarek Sakakini and Suma Bhat and Jinjun Xiong},
    year={2019},
    eprint={1903.10675},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{kurita_2019, 
    title={Paper Dissected: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" Explained}, 
    url={http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/}, 
    journal={Machine Learning Explained}, 
    author={Kurita Keita}, 
    year={2019}, 
    month={Jan}
}

@misc{liu2019roberta,
    title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{
    nayak_2019, 
    title={Understanding searches better than ever before}, 
    url={https://blog.google/products/search/search-language-understanding-bert/}, 
    journal={Google}, 
    publisher={Google}, 
    author={Nayak, Pandu}, 
    year={2019}, 
    month={Oct}
}

@misc{reimers2019sentencebert,
    title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
    author={Nils Reimers and Iryna Gurevych},
    year={2019},
    eprint={1908.10084},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{sanh2019distilbert,
    title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
    author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
    year={2019},
    eprint={1910.01108},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{wolf2019huggingfaces,
    title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
    author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Jamie Brew},
    year={2019},
    eprint={1910.03771},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{yang2019simple,
    title={Simple Applications of BERT for Ad Hoc Document Retrieval},
    author={Wei Yang and Haotian Zhang and Jimmy Lin},
    year={2019},
    eprint={1903.10972},
    archivePrefix={arXiv},
    primaryClass={cs.IR}
}

