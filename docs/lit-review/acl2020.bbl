\begin{thebibliography}{11}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Cer et~al.(2018)Cer, Yang, yi~Kong, Hua, Limtiaco, John, Constant,
  Guajardo-Cespedes, Yuan, Tar, Sung, Strope, and Kurzweil}]{cer2018universal}
Daniel Cer, Yinfei Yang, Sheng yi~Kong, Nan Hua, Nicole Limtiaco, Rhomni~St.
  John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
  Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. 2018.
\newblock \href {http://arxiv.org/abs/1803.11175} {Universal sentence encoder}.

\bibitem[{Clement et~al.(2019)Clement, Bierbaum, O'Keeffe, and
  Alemi}]{clement2019use}
Colin~B. Clement, Matthew Bierbaum, Kevin~P. O'Keeffe, and Alexander~A. Alemi.
  2019.
\newblock \href {http://arxiv.org/abs/1905.00075} {On the use of arxiv as a
  dataset}.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock \href {http://arxiv.org/abs/1810.04805} {Bert: Pre-training of deep
  bidirectional transformers for language understanding}.

\bibitem[{Gong et~al.(2019)Gong, Sakakini, Bhat, and Xiong}]{gong2019document}
Hongyu Gong, Tarek Sakakini, Suma Bhat, and Jinjun Xiong. 2019.
\newblock \href {http://arxiv.org/abs/1903.10675} {Document similarity for
  texts of varying lengths via hidden topics}.

\bibitem[{Keita(2019)}]{kurita_2019}
Kurita Keita. 2019.
\newblock \href
  {http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/}
  {Paper dissected: "bert: Pre-training of deep bidirectional transformers for
  language understanding" explained}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock \href {http://arxiv.org/abs/1907.11692} {Roberta: A robustly
  optimized bert pretraining approach}.

\bibitem[{Nayak(2019)}]{nayak_2019}
Pandu Nayak. 2019.
\newblock \href
  {https://blog.google/products/search/search-language-understanding-bert/}
  {Understanding searches better than ever before}.

\bibitem[{Reimers and Gurevych(2019)}]{reimers2019sentencebert}
Nils Reimers and Iryna Gurevych. 2019.
\newblock \href {http://arxiv.org/abs/1908.10084} {Sentence-bert: Sentence
  embeddings using siamese bert-networks}.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock \href {http://arxiv.org/abs/1910.01108} {Distilbert, a distilled
  version of bert: smaller, faster, cheaper and lighter}.

\bibitem[{Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, and Brew}]{wolf2019huggingfaces}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, and
  Jamie Brew. 2019.
\newblock \href {http://arxiv.org/abs/1910.03771} {Huggingface's transformers:
  State-of-the-art natural language processing}.

\bibitem[{Yang et~al.(2019)Yang, Zhang, and Lin}]{yang2019simple}
Wei Yang, Haotian Zhang, and Jimmy Lin. 2019.
\newblock \href {http://arxiv.org/abs/1903.10972} {Simple applications of bert
  for ad hoc document retrieval}.

\end{thebibliography}
